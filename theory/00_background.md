# Background: How AF-FCL Works

This document explains the mathematics of the original AF-FCL paper (ICLR 2024) step by step.

---

## The Setup

We have **K clients** and **T sequential tasks**. At task t, client k has local data D_k^t = {(x_i, y_i)}. After training on task t, the data is **discarded** (continual learning constraint). A central server aggregates model parameters after each round.

Two challenges:
1. **Catastrophic forgetting**: training on task t overwrites knowledge from tasks 1..t-1.
2. **Heterogeneity**: different clients see different class subsets, so naive model averaging causes interference.

---

## Architecture

The model has two components:

### 1. Classifier f = f_c(h_a(x))

- **Feature extractor** h_a : R^{d_in} -> R^D (CNN or ResNet backbone), where D = 512.
- **Classification head** f_c : R^D -> R^C (fully connected + softmax), where C = total classes.

### 2. Conditional Normalizing Flow g

A generative model that learns the distribution of features h_a(x) conditioned on class label y. It defines a bijective (invertible) mapping between feature space and a latent space where the base distribution is a standard Gaussian N(0, I).

---

## Normalizing Flows: The Core Math

A normalizing flow is an invertible function g that maps between a simple distribution (standard Gaussian) and a complex one (the feature distribution).

### Density Computation (Eq. 3)

Given base distribution p_u(u) = N(0, I) and invertible transform g(.|y), the density of a point z in feature space is:

$$
p_z(z | y) = p_u(g(z, y)) \cdot \left| \det \frac{\partial g(z, y)}{\partial z} \right|
$$

In log form:

$$
\log p_z(z | y) = \underbrace{\log p_u(g(z, y))}_{\text{how Gaussian-like is the latent code}} + \underbrace{\log \left| \det \frac{\partial g}{\partial z} \right|}_{\text{volume correction (Jacobian)}}
$$

Since p_u = N(0, I), the first term expands to:

$$
\log p_u(u) = -\frac{D}{2}\log(2\pi) - \frac{1}{2}\|u\|^2
$$

**Sampling**: To generate a fake feature, sample u ~ N(0, I) and compute z = g^{-1}(u, y).

---

## NF Training (Eq. 5)

The NF is trained to maximize the log-likelihood of real features:

$$
\mathcal{L}_{\text{NF}} = -\frac{1}{|D_k^t|} \sum_{(x_i, y_i) \in D_k^t} \log p_z(h_a(x_i), y_i) \;-\; \frac{\lambda_{\text{last}}}{|G_z|} \sum_{(z_j, y_j) \in G_z} \log p_z(z_j, y_j)
$$

- First term: fit NF to current task's real features.
- Second term: replay --- fit NF to features generated by the old NF g'.
- G_z = {z_j = g'^{-1}(u_j, y_j) : u_j ~ N(0, I)} are replayed features.

---

## Credibility Estimation (Eq. 7-8): The Key Idea

Not all generated features are useful. Some come from classes that conflict with the current task. The paper assigns a "credibility" weight to each generated feature.

### Step 1: Map real features to latent space

For each real data point (x_i, y_i):

$$
\bar{u}_i = g(h_a(x_i), y_i)
$$

### Step 2: Fit a diagonal Gaussian per class (Eq. 7)

$$
\mu_c = \frac{1}{n_c} \sum_{y_i = c} \bar{u}_i, \qquad \Sigma_c = \text{diag}\left(\frac{1}{n_c} \sum_{y_i = c} (\bar{u}_i - \mu_c)^2\right)
$$

### Step 3: Score generated features (Eq. 8)

$$
p_{D_k^t}(\bar{u}) = \mathcal{N}(\bar{u}; \mu_c, \Sigma_c) = \prod_{d=1}^{D} \frac{1}{\sqrt{2\pi\sigma_{c,d}^2}} \exp\left(-\frac{(\bar{u}_d - \mu_{c,d})^2}{2\sigma_{c,d}^2}\right)
$$

If the generated feature's latent code is near where local data lives, it gets a high weight. Otherwise, a low weight. This is "accurate forgetting."

---

## Knowledge Distillation (Eq. 6)

To prevent the feature extractor from drifting too far:

$$
\mathcal{L}_{\text{KD}} = \frac{1}{n} \sum_{i=1}^{n} \| h_a(x_i) - h_a'(x_i) \|^2
$$

where h_a' is the frozen feature extractor from the previous task. This is pointwise MSE.

---

## Explore-Forget Weight

$$
k_{\text{explore}} = (1 - \theta) \cdot \bar{p} + \theta
$$

where p_bar is the average credibility and theta is a **fixed** hyperparameter (hand-tuned per dataset).

---

## Final Objective (Eq. 9)

$$
\mathcal{L}_{\text{total}} = \underbrace{\mathcal{L}_{\text{CE}}^x}_{\text{real data CE}} + k_{\text{flow}} \cdot \left( \underbrace{k_{\text{explore}} \cdot \mathcal{L}_{\text{CE}}^g}_{\text{generated data CE}} + \underbrace{\mathcal{L}_{\text{KD}}^g}_{\text{output KD on generated}} \right) + \underbrace{\mathcal{L}_{\text{KD}}}_{\text{feature + output KD on real}}
$$

---

## Server Aggregation: FedAvg

$$
\theta_{\text{server}} = \sum_{k=1}^{K} \frac{n_k}{\sum_j n_j} \theta_k
$$

All parameters (classifier and NF) are averaged weighted by each client's data size.
