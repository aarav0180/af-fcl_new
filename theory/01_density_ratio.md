# Feature 1: Density Ratio Credibility Estimation

**CLI flag**: `--use_density_ratio`
**Replaces**: Eq. 8 (absolute density credibility)
**File**: `improved/features/density_ratio.py`

---

## What the Original Paper Does

The paper computes a credibility weight for each generated feature by measuring how likely its latent code is under a local Gaussian (Eq. 8):

$$
w_i^{\text{old}} = p_{D_k^t}(\bar{u}_i) = \mathcal{N}(\bar{u}_i; \mu_c, \Sigma_c)
$$

where:
- The generated feature's latent code is: u_bar_i = g(z_i, y_i), where z_i was generated by sampling u ~ N(0, I) and computing z = g^{-1}(u, y).
- mu_c and Sigma_c are the mean and diagonal covariance of real data latent codes for class c (Eq. 7).

---

## What is Wrong

The generated latent codes u_bar_i are sampled from the base distribution N(0, I). The local Gaussian N(mu_c, Sigma_c) is fit in the same latent space.

**Problem**: Since the NF is trained to map features TO a standard Gaussian, the local Gaussian N(mu_c, Sigma_c) typically has mu_c near 0 and Sigma_c near I. This means:

$$
p_{D_k^t}(\bar{u}_i) \approx \mathcal{N}(\bar{u}_i; 0, I) \quad \text{for most classes}
$$

The credibility weight becomes nearly **uniform across all samples** because both the sampling distribution and the scoring distribution are approximately the same Gaussian. The weight fails to distinguish informative from uninformative generated features.

The absolute density conflates two things:
1. How likely the sample is under the **local data distribution** (what we want).
2. How likely the sample is under the **base distribution** (trivially high by construction, since that is where we sampled from).

---

## The Fix: Density Ratio

Replace absolute density with the density **ratio**:

$$
w_i = \frac{p_{D_k^t}(\bar{u}_i)}{p_{\text{prior}}(\bar{u}_i)} = \frac{\mathcal{N}(\bar{u}_i; \mu_c, \Sigma_c)}{\mathcal{N}(\bar{u}_i; 0, I)}
$$

This measures how much **more likely** a sample is under the local distribution compared to the base distribution.

---

## Derivation in Log Space

The constant -D/2 * log(2*pi) appears in both Gaussians and cancels in the ratio.

**Log local density** (per dimension d):

$$
\log p_{\text{local}}^{(d)}(\bar{u}_d) = -\frac{1}{2}\log\sigma_{c,d}^2 - \frac{(\bar{u}_d - \mu_{c,d})^2}{2\sigma_{c,d}^2}
$$

**Log prior density** (per dimension d):

$$
\log p_{\text{prior}}^{(d)}(\bar{u}_d) = -\frac{1}{2}\bar{u}_d^2
$$

**Log density ratio** (per dimension d):

$$
\log r_d = -\frac{1}{2}\log\sigma_{c,d}^2 - \frac{(\bar{u}_d - \mu_{c,d})^2}{2\sigma_{c,d}^2} + \frac{\bar{u}_d^2}{2}
$$

**Final weight** using geometric mean over dimensions (for numerical stability in D=512 dimensions):

$$
w_i = \exp\left(\frac{1}{D} \sum_{d=1}^{D} \log r_d \right) = \exp\left(\frac{1}{D} \sum_{d=1}^{D} \left[ \frac{\bar{u}_{i,d}^2}{2} - \frac{(\bar{u}_{i,d} - \mu_{c,d})^2}{2\sigma_{c,d}^2} - \frac{1}{2}\log\sigma_{c,d}^2 \right] \right)
$$

The weight is clamped to [exp(-10), exp(10)] for numerical stability.

---

## Interpretation

| Condition | Meaning | Effect |
|-----------|---------|--------|
| w_i > 1 | Sample more likely under local than prior | Informative, upweight |
| w_i < 1 | Sample less likely under local than prior | Uninformative, downweight |
| w_i = 1 | Local and prior overlap perfectly | No extra information |

---

## Connection to Importance Sampling

This is an importance sampling correction. When we sample u_bar ~ p_prior and want to evaluate a quantity under p_local:

$$
\mathbb{E}_{p_{\text{local}}}[f(\bar{u})] = \mathbb{E}_{p_{\text{prior}}}\left[\frac{p_{\text{local}}(\bar{u})}{p_{\text{prior}}(\bar{u})} \cdot f(\bar{u})\right]
$$

The density ratio w_i is exactly this importance weight. The original paper implicitly assumes p_prior = 1 (unnormalized), which biases the estimate.
